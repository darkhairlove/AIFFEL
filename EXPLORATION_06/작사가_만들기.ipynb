{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GatrpVBoh0Gk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GatrpVBoh0Gk",
        "outputId": "b8962810-e9f9-4d78-f8f9-2411fbc5c55c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "2b823a70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b823a70",
        "outputId": "fc0d1998-d876-480d-c17d-0479866a34cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import glob\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/Colab/AIFFEL/day/lyrics/*' \n",
        "\n",
        "txt_list = glob.glob(txt_file_path) \n",
        "\n",
        "raw_corpus = [] \n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담음.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines() \n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "3ddf5a2c",
      "metadata": {
        "id": "3ddf5a2c"
      },
      "outputs": [],
      "source": [
        "# 입력된 문장을\n",
        "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "#     2. 특수문자 양쪽에 공백을 넣고\n",
        "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "#     5. 다시 양쪽 공백을 지웁니다\n",
        "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip() # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
        "    return sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "723730a6",
      "metadata": {
        "id": "723730a6"
      },
      "source": [
        "## 정제된 문자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "341271a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "341271a2",
        "outputId": "e1ef216a-842b-4374-efde-b6560aa63950"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<start> looking for some education <end>',\n",
              " '<start> made my way into the night <end>',\n",
              " '<start> all that bullshit conversation <end>',\n",
              " '<start> i don t even wanna waste your time <end>',\n",
              " '<start> let s just say that maybe <end>',\n",
              " '<start> you could help me ease my mind <end>',\n",
              " '<start> if that s love in your eyes <end>',\n",
              " '<start> it s more than enough <end>',\n",
              " '<start> had some bad love <end>',\n",
              " '<start> ooh , ooh looking for some affirmation <end>']"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 원하지 않는 문장은 건너뜀.\n",
        "    if len(sentence) == 0 : continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    # 토큰의 개수가 15개를 넘어가는 문장 제외\n",
        "    if len(preprocessed_sentence.split()) > 15 : continue\n",
        "    corpus.append(preprocessed_sentence)\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "578c0693",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "578c0693",
        "outputId": "062ec39a-de25-4b65-dcf8-79367708323c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  2 290  28 ...   0   0   0]\n",
            " [  2 219  13 ...   0   0   0]\n",
            " [  2  25  15 ...   0   0   0]\n",
            " ...\n",
            " [  2  44  89 ...   0   0   0]\n",
            " [  2   4  24 ...   3   0   0]\n",
            " [  2  23   9 ...   3   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f0b663e3ed0>\n",
            "(156013, 15)\n"
          ]
        }
      ],
      "source": [
        "def tokenize(corpus):\n",
        "  \n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)\n",
        "print(tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "d0feda53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0feda53",
        "outputId": "e75d027d-6c76-4ad1-a52c-d99aea024780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   2  290   28   94 4486    3    0    0    0    0    0    0    0    0]\n",
            "[ 290   28   94 4486    3    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "src_input = tensor[:, :-1]  \n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "bd25932b",
      "metadata": {
        "id": "bd25932b"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "c3ca674e",
      "metadata": {
        "id": "c3ca674e"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=34)\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset_train = dataset_train.shuffle(BUFFER_SIZE)\n",
        "dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "dataset_val = dataset_val.shuffle(BUFFER_SIZE)\n",
        "dataset_val = dataset_val.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "5921321e",
      "metadata": {
        "id": "5921321e"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__() \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "  \n",
        "embedding_size = 512\n",
        "hidden_size = 2048\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) # tokenizer.num_words에 +1인 이유는 문장에 없는 pad가 사용되었기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "495c1504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "495c1504",
        "outputId": "9a57f8a3-eb45-429c-850d-c12829362bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "487/487 [==============================] - 35s 38ms/step - loss: 3.3343 - val_loss: 2.9135\n",
            "Epoch 2/10\n",
            "487/487 [==============================] - 18s 37ms/step - loss: 2.7563 - val_loss: 2.6605\n",
            "Epoch 3/10\n",
            "487/487 [==============================] - 18s 37ms/step - loss: 2.4538 - val_loss: 2.4851\n",
            "Epoch 4/10\n",
            "487/487 [==============================] - 18s 37ms/step - loss: 2.1614 - val_loss: 2.3490\n",
            "Epoch 5/10\n",
            "487/487 [==============================] - 18s 37ms/step - loss: 1.8813 - val_loss: 2.2505\n",
            "Epoch 6/10\n",
            "487/487 [==============================] - 18s 37ms/step - loss: 1.6225 - val_loss: 2.1830\n",
            "Epoch 7/10\n",
            "487/487 [==============================] - 18s 37ms/step - loss: 1.4018 - val_loss: 2.1422\n",
            "Epoch 8/10\n",
            "487/487 [==============================] - 18s 37ms/step - loss: 1.2279 - val_loss: 2.1302\n",
            "Epoch 9/10\n",
            "487/487 [==============================] - 18s 37ms/step - loss: 1.1060 - val_loss: 2.1403\n",
            "Epoch 10/10\n",
            "487/487 [==============================] - 18s 37ms/step - loss: 1.0336 - val_loss: 2.1582\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0b63056990>"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam() \n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none' \n",
        ")\n",
        "tf.config.list_physical_devices('GPU')\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(dataset_train, epochs=10, validation_data=dataset_val, batch_size=256) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qIEWNFmwrU5k",
      "metadata": {
        "id": "qIEWNFmwrU5k"
      },
      "source": [
        "embedding_size = 512 /\n",
        "hidden_size = 2048\n",
        "- val_loss : 2.16\n",
        "- val_loss : 2.15\n",
        "\n",
        "embedding_size = 64 /\n",
        "idden_size  = 1026\n",
        "- val_loss : 2.6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "e2d76997",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2d76997",
        "outputId": "4c1e0ad8-a4d2-467d-feaf-9e01718fcdcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"text_generator_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     multiple                  6144512   \n",
            "                                                                 \n",
            " lstm_14 (LSTM)              multiple                  20979712  \n",
            "                                                                 \n",
            " lstm_15 (LSTM)              multiple                  33562624  \n",
            "                                                                 \n",
            " dense_7 (Dense)             multiple                  24590049  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85,276,897\n",
            "Trainable params: 85,276,897\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "4fc7030b",
      "metadata": {
        "id": "4fc7030b"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    \n",
        "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성성\n",
        "        predict = model(test_tensor) \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated #최종적으로 모델이 생성한 문장을 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "35190c7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "35190c7f",
        "outputId": "e760e457-9bde-4877-d1b2-023e08993513"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> l love can u turn me in a deep <end> '"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> l love\", max_len=20)\n",
        "# generate_text 함수에 lyricist 라 정의한 모델을 이용해서 ilove 로 시작되는 문장을 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yvKfXhjNH7FE",
      "metadata": {
        "id": "yvKfXhjNH7FE"
      },
      "source": [
        "## 회고\n",
        "- 어려웠던 점 : embedding_size와 hidden_size를 얼마만큼 늘려야 loss를 줄일지가 어려웠다. model.fit의 인자를 batch_size만 추가했다.\n",
        "- 알아낸 점 및 모호한 점 : model.fit의 shuffle를 추가한 결과랑 아닌 결과랑 차이점이 있는지 궁금하다. 이미 ataset_train.shuffle()을 통해서 model.fit에서 shuffle을 안해도 되는건지 궁금하다. 출력된 문장에 < start >와 < end >, < unk >는 출력을 안하는 방법이 있는지 궁금하다.\n",
        "- 노력한 점 :  embedding_size와 hidden_size 외에도 다른 model.fit의 인자를 추가하는 방법을 찾기 위해 노력했다. 토큰화 했을 때, 문장 길이가 15개 이하만 train데이터로 하는 과정을 찾아 넣었다.\n",
        "- 자기다짐 :  embedding_size와 hidden_size을 여러 가지로 해보는 경험이 중요하다고 느꼈다. train과 test 를 split에서 test data의 비율은 고정이고 radomstate를 바꿨을때 val_loss의 값은 크게 차이가 없다는 것을 보았을 때, embedding과 hidden size가 중요하다는 것을 알 수 있었다. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
